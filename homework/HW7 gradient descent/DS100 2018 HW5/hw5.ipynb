{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Lastly, hit **Validate**.\n",
    "\n",
    "If you worked locally, and then uploaded your work to the hub, make sure to follow these steps:\n",
    "- open your uploaded notebook **on the hub**\n",
    "- hit the validate button right above this cell, from inside the notebook\n",
    "\n",
    "These  steps should solve any issue related to submitting the notebook on the hub.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99241281e589ab3c08a8d3b926a35ae1",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 5: Modeling, Estimation and Gradient Descent\n",
    "## Course Policies\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** at the top of your solution.\n",
    "\n",
    "## Due Date: 11:59pm Wednesday, March 21\n",
    "\n",
    "## Introduction\n",
    "In this homework, we explore modeling data, estimating optimal parameters and a numerical estimation method, gradient descent. These concepts are some of the fundamentals of data science and machine learning and will serve as the building blocks for future projects, classes, and work.\n",
    "\n",
    "After this homework, you should feel comfortable with the following:\n",
    "\n",
    "1. Practice reasoning about a model\n",
    "\n",
    "2. Build some intuition for loss functions and how the behave \n",
    "\n",
    "3. Work through deriving the gradient of a loss with respect to model parameters\n",
    "\n",
    "4. Work through a basic version of gradient descent.\n",
    "\n",
    "This homework is comprised of completing code, deriving analytic solutions, writing LaTex and visualizing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c47e278e09d6b09f99edf34faa82fd31",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0de6ac714e983580ceea233f5986eccd",
     "grade": false,
     "grade_id": "utils-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from hw5_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e50d87c58433d1f61ccb5dbb0909c77a",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66fb081dbb700fef1da938cce15036c7",
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv(\"http://www.ds100.org/sp18/assets/datasets/hw5_data.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c91b0906fb8287c44a7a663253adbf7a",
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. A Simple Model\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3603a428505555ab1d6525b37450f8b",
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1a\n",
    "First, let's visualize the data in a scatter plot. After implementing the `scatter` function below, you should see something like this:\n",
    "![scatter](scatter.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "031194d8ce49fedb13c7b92b1a41d093",
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6bf948413ee2c6277b1e0297a62ebe4",
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1b\n",
    "Describe any significant observations about the distribution of the data. How can you describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a50b320b61cf673999e00ac12ea7909",
     "grade": true,
     "grade_id": "q1b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "I observe ...\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "299421c38e8ce4106c697c6c0f73f479",
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1c\n",
    "The data looks roughly linear, with some extra noise. For now, let's assume that the data follows some linear model, parametrized by $\\theta$:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Define a linear model function that estimates a value $\\hat{y}$ given $x$ and $\\theta$. This model is similar to the model you defined in Lab 8.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f0a72bcf1a6f5db569d4fa14c88e2f5",
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e36884f54d707ac74716f6bb333fb8b",
     "grade": true,
     "grade_id": "q1c-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert linear_model(0, 1) == 0\n",
    "assert linear_model(10, 10) == 100\n",
    "assert np.sum(linear_model(np.array([3, 5]), 3)) == 24\n",
    "assert linear_model(np.array([7, 8]), 4).mean() == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae7ab3ef4cc512c35ec71d3936de6519",
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1d\n",
    "In class, we learned that the $L^2$ (or squared) loss function is smooth and continuous. Let's use $L^2$ loss to evaluate our estimate on $\\theta$ and predict an optimal value for $\\theta$, named $\\theta^*$. Define the $L^2$ loss function `l2_loss` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "140a007e3b6ca72ce415efd50b245ac8",
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the average l2 loss given y and y_hat\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e52031bf14b7bbf16073d8c35dcd9a2",
     "grade": true,
     "grade_id": "q1d-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert l2_loss(2, 1) == 1\n",
    "assert l2_loss(2, 0) == 4 \n",
    "assert l2_loss(5, 1) == 16\n",
    "assert l2_loss(np.array([5, 6]), np.array([1, 1])) == 20.5\n",
    "assert l2_loss(np.array([1, 1, 1]), np.array([4, 1, 4])) == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96544f4c36a79984e5de4a4a487c31eb",
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1e\n",
    "\n",
    "First, visualize the $L^2$ loss as a function of $\\theta$, where several different values of $\\theta$ are given. Be sure to label your axes properly. You plot should look something like this:\n",
    "![avg_l2](l2_avg_loss.png)\n",
    "\n",
    "What looks like the optimal $\\theta^*$ value based on the visualization? Set `theta_star_guess` to the value of $\\theta$ that appears to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3816fa8ede79de1b481d6f7583f844f6",
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- the vector containing different estimates of theta\n",
    "    \"\"\"\n",
    "    avg_loss = ... # Calculate the loss here for each value of theta\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    ... # Create your plot here\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)\n",
    "\n",
    "theta_star_guess = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed066111e2c8396152c7fa9154367ad5",
     "grade": true,
     "grade_id": "q1e-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert l2_loss(3, 2) == 1\n",
    "assert l2_loss(0, 10) == 100\n",
    "assert 1 <= theta_star_guess <= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca181f0e94b22634effdb43518e0829",
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Fitting our Simple Model\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "### Question 2a\n",
    "Let's confirm our visual findings for optimal $\\theta^*$. First, find the analytical solution for the optimal $\\theta^*$ for average $L^2$ loss. Write up your solution in the cell below using LaTex. Here are some useful examples of LaTex syntax:\n",
    "\n",
    "Summation: $\\sum_{i=1}^n a_i$\n",
    "\n",
    "Exponent: $a^2$\n",
    "\n",
    "Fraction: $\\frac{a}{b}$\n",
    "\n",
    "Multiplication: $a \\cdot b$\n",
    "\n",
    "Derivative: $\\frac{\\partial}{\\partial a}$\n",
    "\n",
    "Symbols: $\\alpha, \\beta, \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19b48e5010dd894ab2564508ca3f3556",
     "grade": true,
     "grade_id": "q2a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "The solution is:\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c598f919a78d4641871731a7612b2883",
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2b\n",
    "Now that we have the analytic solution for $\\theta^*$, implement the function `find_theta` that calculates the numerical value of $\\theta^*$ based on our data $x$, $y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bdfd0c1ee789011fb6867d7a1f4234",
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    theta_opt = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27647b637e875e3bf095a08e5195ec36",
     "grade": true,
     "grade_id": "q2b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t_star = find_theta(x, y)\n",
    "print(f'theta_opt = {t_star}')\n",
    "\n",
    "assert 1.4 <= t_star <= 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d229dae97b016b9810da4ef0b1eb5b4a",
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2c\n",
    "Now, let's plot our loss function again using the `visualize` function. But this time, add a vertical line at the optimal value of theta (plot the line $x = \\theta^*$). Your plot should look something like this:\n",
    "![vertical_linear](vertical_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6dfba540f44324f8e9d36b23c6d89b",
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = ...\n",
    "...\n",
    "...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0ad935625ba2fdf262b65914e1d8bb0",
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2d\n",
    "We now have an optimal value for $\\theta$ that minimizes our loss. In the cell below, plot the scatter plot of the data from Question 1a (you can reuse the `scatter` function here). But this time, add the line $\\hat{y} = \\theta^* \\cdot x$ using the $\\theta^*$ you computed above. Your plot should look something like this:\n",
    "![scatter_with_line](scatter_with_line.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de409debacae960ce0881344523b1727",
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = ...\n",
    "...\n",
    "...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850b8b8fb3a916556a9748579d7535e3",
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2e\n",
    "Great! It looks like our estimate for $\\theta$ is able to capture a lot of the data with a single parameter. Now let's try to remove the linear portion of our model from the data to see if we missed anything. \n",
    "\n",
    "The remaining data is known as the residual, $r=y-\\theta^* \\cdot x$. Below, write a function to find the residual and plot the residuals corresponding to x in a scatter plot. Plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63783515265349fca282efe28c0e5253",
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdd940e38928a89f1e6bc9709a2dd8c0",
     "grade": false,
     "grade_id": "q2f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2f\n",
    "What does the residual look like? Do you notice a relationship between $x$ and $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02efdbabc1e2b51e21ec06fb015533e3",
     "grade": true,
     "grade_id": "q2f-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "I observe that...\n",
    "    \n",
    "    \n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d0bb728baaa970ecddb9d55cb73057",
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Increasing Model Complexity\n",
    "\n",
    "It looks like the remaining data is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "\\hat{y} = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\vec{\\theta}$.\n",
    "\n",
    "Note that a generalized sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. We can assume that the scaling and shifting parameter ($a$ and $c$ in this case) are 1 and 0 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7100f11e2a64f7f6080c68ea75cba2",
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3a\n",
    "In the following cell, **explain why we can assume the scaling parameter to be 1 and shifting parameter to be 0 based on the residual plot in Question 2e**. \n",
    "\n",
    "You might find the following code helpful in visualizing all three parameters.\n",
    "\n",
    "```python\n",
    "def plot_sin_generalized(a,b,c,label=None):\n",
    "    \"\"\"Plot a sin function with three parameters\"\"\"\n",
    "    X = np.linspace(-5, 5)\n",
    "    Y = a * np.sin(b*X + c)\n",
    "    plt.scatter(X, Y, label=label)\n",
    "    plt.legend()\n",
    "```\n",
    "\n",
    "You can try plotting: \n",
    "```python\n",
    "plot_sin_generalized(1,1,1, label='sin(x)')\n",
    "plot_sin_generalized(1,1,2, label='sin(x + 2)')\n",
    "plot_sin_generalized(1,2,2, label='sin(2x + 2)')\n",
    "plot_sin_generalized(2,2,2, label='2sin(2x + 2)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a874e5dc3a1f1b09ba532a4f27f4a91",
     "grade": true,
     "grade_id": "q3a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Your Solution Here:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "589586692cd2c8c57af6909a6b7b2f00",
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3b\n",
    "As in Question 1, write a function that predicts a value $\\hat{y}$ given an input $x$ based on our new model.\n",
    "\n",
    "*Hint:* Try to do this without using for loops. The `np.sin` function may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c51c13902da418ef6f7d928e01b661fb",
     "grade": false,
     "grade_id": "q3b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta_1 -- the scalar value theta_1\n",
    "    theta_2 -- the scalar value theta_2\n",
    "    \"\"\"\n",
    "    y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0027de55ddeae9aedccb74721a702ebc",
     "grade": true,
     "grade_id": "q3b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(sin_model(1, 1, np.pi), 1.0000000000000002)\n",
    "# Check that we accept x as arrays\n",
    "assert len(sin_model(x, 2, 2)) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fb1906b809b3cdc3afbe4357a438a67",
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3c\n",
    "Use the average $L^2$ loss to compute $\\frac{\\partial L }{\\partial \\theta_1}, \\frac{\\partial L }{\\partial \\theta_2}$. \n",
    "\n",
    "First, we will use LaTex to write $L(x, y, \\theta_1, \\theta_2)$, $\\frac{\\partial L }{\\partial \\theta_1}$, and $\\frac{\\partial L }{\\partial \\theta_2}$ given $\\vec{x}$, $\\vec{y}$, $\\vec{\\theta}$. \n",
    "\n",
    "Notice that we now have $\\vec{x}$ and $\\vec{y}$ instead of $x$ and $y$. This means that when writing the loss function $L(x, y, \\theta_1, \\theta_2)$, you'll need to take the average of the squared losses for each $y_i$, $\\hat{y_i}$ pair.\n",
    "\n",
    "You don't need to write out the full derivation. Just the final expression is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad480b2f706b2a832004c4115771cfdb",
     "grade": true,
     "grade_id": "q3c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "L(x, y, \\theta_1, \\theta_2) &= \\dots \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta_1} &= \\dots \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta_2} &= \\dots\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f757014e89824096538f8887dd5dcf",
     "grade": false,
     "grade_id": "q3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 3d\n",
    "Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ respectively. Use the formulas you wrote for $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11fb9677bdf44ffc6e48c978ea2318c7",
     "grade": false,
     "grade_id": "q3d-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "546e1b76add5b82ded7e4d7715f65a0d",
     "grade": false,
     "grade_id": "q3d-answer-2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2de473e62f7c86060336072a955c4d9",
     "grade": false,
     "grade_id": "dt",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.vstack([\n",
    "        dt1(x, y, theta),\n",
    "        dt2(x, y, theta)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "842f5d878acccdbd19352feb1d00ef6a",
     "grade": true,
     "grade_id": "q3d-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(dt1(x, y, [0, np.pi]), -25.376660670924529)\n",
    "assert np.isclose(dt2(x, y, [0, np.pi]), 1.9427210155296564)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e4eac82be751c35e57b2901bedbf82",
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4. Gradient Descent\n",
    "Now try to solve for the optimal $\\theta^*$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to alternative optimization methods for finding an approximate solution.\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "### Question 4a\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in array $x$, array $y$, and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step. \n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the $L^2$ loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd1f17360f63ded9d0f0dfe08581039",
     "grade": false,
     "grade_id": "init-t",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 0] as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49963085c264dc13c9405db3f0936b3",
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "030f63d30d7689a302db74c047221beb",
     "grade": true,
     "grade_id": "q4a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t, num_iter=20, alpha=0.1)\n",
    "\n",
    "assert len(ts) == len(loss) == 20 # theta history and loss history are 20 items in them\n",
    "assert ts[0].shape == (2,1) # theta history contains theta values\n",
    "assert np.isscalar(loss[0]) # loss history is a list of scalar values, not vector\n",
    "\n",
    "assert loss[1] - loss[-1] > 0 # loss is decreasing\n",
    "\n",
    "assert np.allclose(np.sum(t_est), 4.5, atol=2e-1)  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9081e44dcaa2e6bd55b19d99e2a8e94d",
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4b\n",
    "Now, let's try using a decaying learning rate. Implement `grad_desc_decay` below, which performs gradient descent with a learning rate that decreases slightly with each time step. You should be able to copy most of your work from the previous part, but you'll need to tweak how you update `theta` at each time step.\n",
    "\n",
    "By decaying learning rate, we mean instead of just a number $\\alpha$, the learning should be now $\\frac{\\alpha}{i+1}$ where $i$ is the current number of iteration. (Why do we need to add '+ 1' in the denominator?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d0193bf54cb01ff12364f80ad4210b",
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc_decay(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and decaying learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b0d91b37c22bb4f01b3ebca97f1bba",
     "grade": true,
     "grade_id": "q4b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t, num_iter=20, alpha=0.1)\n",
    "\n",
    "assert len(ts_decay) == len(loss_decay) == 20 # theta history and loss history are 20 items in them\n",
    "assert ts_decay[0].shape == (2,1) # theta history contains theta values\n",
    "assert np.isscalar(loss[0]) # loss history should be a list of values, not vector\n",
    "\n",
    "assert loss_decay[1] - loss_decay[-1] > 0 # loss is decreasing\n",
    "\n",
    "assert np.allclose(np.sum(t_est_decay), 4.5, atol=2e-1)  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7a4de4c820b43095f4a209102836032",
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4c\n",
    "Let's visually inspect our results of running gradient descent to optimize $\\theta$. Plot our x values with our model's predicted y values over the original scatter plot. Did gradient descent successfully optimize $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e435cdf01d7746aa75f8c02102c5cf19",
     "grade": false,
     "grade_id": "q4c-answer",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)\n",
    "\n",
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c4757865c7f7455890131927bd0ee5a",
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, t_est[0], t_est[1])\n",
    "\n",
    "plt.plot(x, y_pred, label='Model')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12706d520b488ac0538bc4f722857ce",
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4d\n",
    "Let's compare our two gradient descent methods and see how they differ. Plot the loss values over each iteration of gradient descent for both static learning rate and decaying learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "167b34d9b23f0e80454f7496371f07f3",
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(...) # Plot of loss history for static learning rate\n",
    "# plt.plot(...) # Plot of loss history for decaying learning rate\n",
    "...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d9f1c8753fb61ffb493a5cdea66712d",
     "grade": false,
     "grade_id": "q4e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 4e\n",
    "Compare and contrast the performance of the two gradient descent methods. Which method begins to converge more quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7817a1e2e9b92ca01299629f7b27e3b0",
     "grade": true,
     "grade_id": "q4e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "I observed that ...\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a74a22dad9b8efbcc19133a2ff887b6",
     "grade": false,
     "grade_id": "loss-3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 5. Visualizing Loss\n",
    "\n",
    "### Question 5a: 3D Plot\n",
    "In the previous plot is about the loss decrease over time, but what exactly is path the theta value? Run the following three cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d57683f5a0862beb13edb374dc5ef5",
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "ts = np.array(ts).squeeze()\n",
    "ts_decay = np.array(ts_decay).squeeze()\n",
    "loss = np.array(loss)\n",
    "loss_decay = np.array(loss_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2239c32003646be003bd1364d9c28266",
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(ts[:, 0], ts[:, 1], loss, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d0ca51ea241c1c961a07fb616d3a935",
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(ts_decay[:, 0], ts_decay[:, 1], loss_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e13210157375396c7e56fd444ae341c",
     "grade": false,
     "grade_id": "q5a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cell, write 1-2 sentences about the differences between using a static learning rate and a learning rate with decay for gradient descent. Use the loss history plot as well as the two 3D visualization to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4b8b49598b6841a3692f7b6de7c0a99",
     "grade": true,
     "grade_id": "q5a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "196249bb13fe10b42d9fc7ba62f229b1",
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5b: Contour Plot\n",
    "\n",
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. \n",
    "\n",
    "Please refer to this notebook when you are working on the next question: http://www.ds100.org/fa17/assets/notebooks/26-lec/Logistic_Regression_Part_2.html#Visualizing-the-gradient-descent-path\n",
    "\n",
    "In next question, fill in the necessary part to create a contour plot. Then run the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebc4018bfe198eda5a49a3df8363e719",
     "grade": false,
     "grade_id": "q5b-import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fcb9a5dece90f241eaddb366fb22d56",
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    thata_points = go.Scatter(name=\"Theta Values\", \n",
    "                              x=..., #TODO\n",
    "                              y=..., #TODO\n",
    "                              mode=\"lines+markers\")\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    t1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    t2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(t1_s, t2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for t1, t2 in data:\n",
    "        l = loss_function(model(x, t1, t2), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create the contour \n",
    "    lr_loss_contours = go.Contour(x=..., #TODO \n",
    "                                  y=..., #TODO\n",
    "                                  z=..., #TODO\n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, thata_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad8d7e0db145b1bd8867c927fca3b57",
     "grade": false,
     "grade_id": "q5b-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "contour_plot('Gradient Descent with Static Learning Rate', ts, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9347b80c2cfb388edffb528e415deed4",
     "grade": false,
     "grade_id": "q5b-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "contour_plot('Gradient Descent with Decay Learning Rate', ts_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f0290fe23dd0dffe8df72ef9be17f8",
     "grade": false,
     "grade_id": "q5b-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cells, write down the answer to the following questions:\n",
    "- How do you interpret the two contour plots? \n",
    "- Compare contour plot and 3D plot, what are the pros and cons of each? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a52ad03c789dd2805164ff2163608eb4",
     "grade": true,
     "grade_id": "q5b-5",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9640453c10330d7c299326978eea18f7",
     "grade": false,
     "grade_id": "improvements",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## How to Improve?\n",
    "It looks like our model, a combination of a linear function and sinusoidal function, was able to almost perfectly fit our data. It turns out that many real world scenarios come from relatively simple models. At the same time, the real world can be incredibly complex and a simple model wouldn't work so well. Consider the example below; it is neither linear, nor sinusoidal, nor quadratic. Suggest how we could iteratively create a model to fit this data and how we might improve our results. Try fitting the data below if you have time :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "102bfb39186aad88848d466a66a14c27",
     "grade": false,
     "grade_id": "improvements-model",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for t in np.linspace(0,10*np.pi, 200):\n",
    "    r = ((t)**2)\n",
    "    x.append(r*np.cos(t))\n",
    "    y.append(r*np.sin(t))\n",
    "\n",
    "plt.scatter(x,y)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d50ed7b161e3027323bf6114e2cd167",
     "grade": true,
     "grade_id": "improvements-suggestion",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c651b7acafc2f945fedaf15e5387fab",
     "grade": false,
     "grade_id": "submit",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "You're done!\n",
    "\n",
    "In order to turn in this assignment, submit this notebook to the Data 100 datahub at http://data100.datahub.berkeley.edu. \n",
    "\n",
    "You will need to upload this notebook and any associated files to datahub manually if you have completed this assignment on your local machine. Detailed instructions for how to submit on Datahub can be found at http://www.ds100.org/sp18/materials.\n",
    "\n",
    "Remember to click 'Validate' for this assignment before submitting. After clicking 'Submit', you can verify there is a time-stamped submission under 'Submitted assignments'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
